import requests
import os
import re
import fitz  # pymupdf para leer PDFs
import numpy as np
from config import EMBEDDING_API_URL, es, headers


def create_index(index):
    """Funci√≥n para crear un √≠ndice en Elasticsearch."""
    index_name = index

    if not es.indices.exists(index=index_name):
        es.indices.create(index=index_name)

def vectorizar_texto(text, max_chars=150000):
    """Funci√≥n que utiliza la API de NextAI para vectorizar un texto, dividi√©ndolo si excede el l√≠mite de caracteres."""

    text = limpiar_texto(text)
    if not text:
        print("Error: El texto qued√≥ vac√≠o despu√©s de la limpieza")
        return None
    
    # Si el texto es m√°s corto que el m√°ximo, procesarlo directamente
    if len(text) <= max_chars:
        body = {
            "texts": [text],
            "model": "LaBSE"
        }
        try:
            response = requests.post(EMBEDDING_API_URL, headers=headers, json=body)
            if response.status_code == 200:
                return response.json()['vectors'][0]
            else:
                print(f"Error en la solicitud. C√≥digo de respuesta: {response.status_code}")
                return None
        except Exception as e:
            print(f"Excepci√≥n al hacer la solicitud: {str(e)}")
            return None
    
    # Dividir el texto en segmentos usando puntos como separadores
    segments = []
    current_text = text
    while len(current_text) > max_chars:
        # Buscar el √∫ltimo punto antes del l√≠mite
        split_index = current_text.rfind('.', 0, max_chars)
        
        if split_index == -1:
            # Si no hay punto, buscar el √∫ltimo espacio
            split_index = current_text.rfind(' ', 0, max_chars)
            if split_index == -1:
                # Si no hay espacio, cortar en el l√≠mite
                split_index = max_chars
        
        segment = current_text[:split_index].strip()
        if segment:
            segments.append(segment)
        current_text = current_text[split_index + 1:].strip()
    
    if current_text:
        segments.append(current_text)
    
    print(f"Texto dividido en {len(segments)} segmentos")
    
    # Vectorizar cada segmento
    vectors = []
    for i, segment in enumerate(segments, 1):
        print(f"\nProcesando segmento {i}/{len(segments)}")
        print(f"Longitud del segmento: {len(segment)} caracteres")
        
        # Asegurarse de que el segmento est√© limpio
        segment = limpiar_texto(segment)
        if not segment:
            print(f"Segmento {i} qued√≥ vac√≠o despu√©s de la limpieza")
            continue
            
        body = {
            "texts": [segment],
            "model": "LaBSE"
        }
        
        try:
            response = requests.post(EMBEDDING_API_URL, headers=headers, json=body)
            if response.status_code == 200:
                vectors.append(response.json()['vectors'][0])
                print(f"‚úÖ Segmento {i} procesado correctamente")
            else:
                print(f"‚ùå Error en segmento {i}. C√≥digo: {response.status_code}")
                
                # Intentar dividir el segmento si sigue siendo muy largo
                if len(segment) > max_chars // 2:
                    print("Intentando dividir el segmento...")
                    subsegments = [segment[j:j + max_chars // 2] for j in range(0, len(segment), max_chars // 2)]
                    for subseg in subsegments:
                        subseg = limpiar_texto(subseg)
                        if subseg:
                            body["texts"] = [subseg]
                            response = requests.post(EMBEDDING_API_URL, headers=headers, json=body)
                            if response.status_code == 200:
                                vectors.append(response.json()['vectors'][0])
                                print(f"‚úÖ Subsegmento procesado correctamente")
                
        except Exception as e:
            print(f"Error en segmento {i}: {str(e)}")
            continue
    
    if not vectors:
        print("‚ùå No se pudo vectorizar ning√∫n segmento")
        return None
    
    # Calcular el vector promedio de los segmentos
    return np.mean(vectors, axis=0).tolist()


def limpiar_texto(texto):
    """
    Funci√≥n auxiliar para limpiar y formatear el texto.
    """
    # Reemplazar m√∫ltiples espacios y saltos de l√≠nea con un solo espacio
    texto = re.sub(r'\s+', ' ', texto)
    
    # Eliminar caracteres no imprimibles excepto espacios normales
    texto = ''.join(char for char in texto if char.isprintable() or char == ' ')
    
    # Eliminar espacios al inicio y final
    texto = texto.strip()
    
    # Asegurarse de que el texto no est√© vac√≠o
    if not texto:
        return None
        
    return texto

def extract_text_from_pdf(pdf_path):
    """Funci√≥n que lee un PDF y extrae el texto."""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text") + "\n"
    return text.strip()

def index_pdf(pdf_path, index_name="documents"):
    text = extract_text_from_pdf(pdf_path)
    print(f"Longitud del texto extra√≠do: {len(text)} caracteres")
    
    # Verificar caracteres especiales en el texto completo
    special_chars = set(char for char in text[:1000] if not char.isprintable())
    if special_chars:
        print(f"Caracteres especiales encontrados en el texto: {special_chars}")

    embedding = vectorizar_texto(text)

    if embedding:
        doc = {
            "filename": os.path.basename(pdf_path),
            "content": text,
            "vector": embedding  # Guardamos el embedding generado por la API
        }
        es.index(index=index_name, body=doc)  # Indexamos en Elasticsearch
        print(f"‚úÖ {pdf_path} indexado correctamente")
    else:
        print(f"‚ö†Ô∏è No se pudo indexar {pdf_path}")

def close_connection():
    """Funci√≥n para cerrar la conexi√≥n con Elasticsearch."""
    es.close()
    print("Conexi√≥n cerrada con Elasticsearch")

if __name__ == "__main__":
    indice = "textos_prueba"  
    create_index(indice)

    pdf_folder = r"C:\Users\raulc\OneDrive - UNIR\Documentos\ViewNext\MasterVN\Proyecto\Docs"  
    for pdf_file in os.listdir(pdf_folder):
        print(f"Indexando {pdf_file}")
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, pdf_file)
            index_pdf(pdf_path)

    print("üìÑ Todos los PDFs han sido indexados en Elasticsearch üöÄ")
    close_connection()